---
title: Tokenizer 정리-3
author: kiwing
date: 2023-07-23 22:51:21 +0800
categories: [AI, technique]
tags: [technique]
pin: false
---

#### [Tokenizer 정리-1](https://ki-wing.github.io/posts/tokenizer/)
#### [Tokenizer 정리-2](https://ki-wing.github.io/posts/tokenizer2/)
#### [Tokenizer 정리-3](https://ki-wing.github.io/posts/tokenizer3/)

## SentencePiece (SPM)
- 논문 : https://arxiv.org/pdf/1808.06226.pdf
- No pre-tokenization required (Available regardless of language)
- Character-coverage can be set
- Subword generalzation
- ALBERT, XLNet, Marian, and T5

<br>

## 동작 코드

- 예제 dataset 가져오기

```python
import sentencepiece as spm
import pandas as pd
import urllib.request
import csv

urllib.request.urlretrieve("https://raw.githubusercontent.com/LawrenceDuan/IMDb-Review-Analysis/master/IMDb_Reviews.csv", filename="IMDb_Reviews.csv")

train_df = pd.read_csv('IMDb_Reviews.csv')
train_df['review']

```

<img src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/b6c159be-eed3-4997-bafc-c3792e83d87e" alt="1" width="100%"/>

<br>

<details>
<summary>코퍼스 정보</summary> 

    - input : 학습시킬 파일
    - model_prefix : 만들어질 모델 이름
    - vocab_size : 단어 집합의 크기
    - model_type : 사용할 모델 (unigram(default), bpe, char, word)
    - max_sentence_length: 문장의 최대 길이
    - pad_id, pad_piece: pad token id, 값
    - unk_id, unk_piece: unknown token id, 값
    - bos_id, bos_piece: begin of sentence token id, 값
    - eos_id, eos_piece: end of sequence token id, 값
    - user_defined_symbols: 사용자 정의 토큰

</details>

```python
print('리뷰 개수 :',len(train_df))

with open('imdb_review.txt', 'w', encoding='utf8') as f:
    f.write('\n'.join(train_df['review']))

spm.SentencePieceTrainer.Train('--input=imdb_review.txt --model_prefix=imdb --vocab_size=5000 --model_type=bpe --max_sentence_length=9999')
```

<img src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/4c6f96cc-1723-4ce3-a6d4-7849e2c04151" alt="2" width="100%"/>

<br>

```python
sp = spm.SentencePieceProcessor()
vocab_file = "imdb.model"
sp.load(vocab_file)

#encode_as_pieces : 문장을 입력하면 서브 워드 시퀀스로 변환
#encode_as_ids : 문장을 입력하면 정수 시퀀스로 변환

lines = [
    "________",
  "I didn't at all think of it _this ___way.",
  "I have waited a long time for someone to film"
]
for line in lines:
    print(line)
    print(sp.encode_as_pieces(line))
    print(sp.encode_as_ids(line))
    print()

sp.GetPieceSize() # 단어 집합 크기 확인 5000
sp.IdToPiece(430) # 정수 -> 서브 워드로  '▁character'
sp.PieceToId('▁character')   # 서브워드 -> 정수 430
sp.DecodeIds([41, 141, 1364, 1120, 4, 666, 285, 92, 1078, 33, 91]) # 정수 시퀀스 -> 문장  'Iul wa fall aold timeooland to film'
sp.DecodePieces(['▁I', '▁have', '▁wa', 'ited', '▁a', '▁long', '▁time', '▁for', '▁someone', '▁to', '▁film'])  # 서브워드 시퀀스로부터 문장으로 변환
# 'I have waited a long time for someone to film'
```

<br>

## Unigram
- 논문 : https://arxiv.org/pdf/1804.10959.pdf
- Calculate the loss for each subword

<img src="https://Ki-Wing.github.io/assets/img/ai_paper/token/5.png" width="115%" alt="3" align="center"/>

<br>

### 계산

["p", "u", "g"] : 0.000389

["p", "ug"] : 0.0022676

["pu", "g"] : 0.0022676


10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8

= log(P(word))

- "hug": ["hu", "g"] (score 0.006802) 
    
- "hugs": ["hu", "gs"] (score 0.001701)

## 동작 코드

- corpus 준비

```python
corpus = [
    "This is the Hugging Face course.",
    "This chapter is about tokenization.",
    "This section shows several tokenizer algorithms.",
    "Hopefully, you will be able to understand how they are trained and generate tokens.",
]
```

- Tokenizer 실행

```python
from transformers import AutoTokenizer
from collections import defaultdict

tokenizer = AutoTokenizer.from_pretrained("xlnet-base-cased")

word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```
<img src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/5912d8e8-380c-4a46-be56-2aa36297b816" alt="4" width="80%" align="center"/>

```python
char_freqs = defaultdict(int)
subwords_freqs = defaultdict(int)
for word, freq in word_freqs.items():
    for i in range(len(word)):
        char_freqs[word[i]] += freq
        for j in range(i + 2, len(word) + 1):
            subwords_freqs[word[i:j]] += freq

sorted_subwords = sorted(subwords_freqs.items(), key=lambda x: x[1], reverse=True)
sorted_subwords[:10]

# [('▁t', 7), ('is', 5), ('er', 5), ('▁a', 5), ('▁to', 4), ('to', 4), ('en', 4), ('▁T', 3), ('▁Th', 3), ('▁Thi', 3)]
```

- word_freqs: records the frequency of occurrence of each word in a given corpus
- char_freqs: records the frequency of occurrence of each character (letters) in a given corpus
- subwords_freqs: records the frequency of occurrence of subwords of words in a given corpus

<br>

```python
from math import log

total_sum = sum([freq for token, freq in token_freqs.items()])
model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}

# Viterbi 
def encode_word(word, model):
    best_segmentations = [{"start": 0, "score": 1}] + [
        {"start": None, "score": None} for _ in range(len(word))
    ]
    for start_idx in range(len(word)):
        # loop 이전 단계에서 적절하게 채워져야함
        best_score_at_start = best_segmentations[start_idx]["score"]
        for end_idx in range(start_idx + 1, len(word) + 1):
            token = word[start_idx:end_idx]
            if token in model and best_score_at_start is not None:
                score = model[token] + best_score_at_start
                if (
                    best_segmentations[end_idx]["score"] is None
                    or best_segmentations[end_idx]["score"] > score
                ):
                    best_segmentations[end_idx] = {"start": start_idx, "score": score}

    segmentation = best_segmentations[-1]
    if segmentation["score"] is None:
        return ["<unk>"], None

    score = segmentation["score"]
    start = segmentation["start"]
    end = len(word)
    tokens = []
    while start != 0:
        tokens.insert(0, word[start:end])
        next_start = best_segmentations[start]["start"]
        end = start
        start = next_start
    tokens.insert(0, word[start:end])
    return tokens, score

print(encode_word("Hopefully", model)) #(['H', 'o', 'p', 'e', 'f', 'u', 'll', 'y'], 41.54264024176184)

def compute_loss(model):
    loss = 0
    for word, freq in word_freqs.items():
        _, word_loss = encode_word(word, model)
        loss += freq * word_loss
    return loss

compute_loss(model)  # 413.362600202517
```

```python
import copy

def compute_scores(model):
    scores = {}
    model_loss = compute_loss(model)
    for token, score in model.items():
        if len(token) == 1:
            continue
        model_without_token = copy.deepcopy(model)
        _ = model_without_token.pop(token)
        scores[token] = compute_loss(model_without_token) - model_loss
    return scores

scores = compute_scores(model)
print(scores["ll"])  # 6.383135099029346
print(scores["his"])  # 0.0


percent_to_remove = 0.1
while len(model) > 100:
    scores = compute_scores(model)
    sorted_scores = sorted(scores.items(), key=lambda x: x[1])
    # Remove percent_to_remove tokens with the lowest scores.
    for i in range(int(len(model) * percent_to_remove)):
        _ = token_freqs.pop(sorted_scores[i][0])

    total_sum = sum([freq for token, freq in token_freqs.items()])
    model = {token: -log(freq / total_sum) for token, freq in token_freqs.items()}

def tokenize(text, model):
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in words_with_offsets]
    encoded_words = [encode_word(word, model)[0] for word in pre_tokenized_text]
    return sum(encoded_words, [])

tokenize("This is the Hugging Face course.", model)
# ['▁This', '▁is', '▁the', '▁Hugging', '▁Face', '▁course.']
```


[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
