---
title: Tokenizer 정리-2
author: kiwing
date: 2023-07-23 22:51:21 +0800
categories: [AI, technique]
tags: [technique]
pin: false
---

#### [Tokenizer 정리-1](https://ki-wing.github.io/posts/tokenizer/)
#### [Tokenizer 정리-2](https://ki-wing.github.io/posts/tokenizer2/)
#### [Tokenizer 정리-3](https://ki-wing.github.io/posts/tokenizer3/)

## WordPiece
- 논문 : https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf
- Merge the pair with the highest Likelihood of the corpus when merged

<img src="https://Ki-Wing.github.io/assets/img/ai_paper/token/3.png" alt="1" width="100%"/>

1. 텍스트의 모든 문자로 단어 단위 인벤토리를 초기화

2. 1의 인벤토리를 사용하여 교육 데이터에 언어 모델을 구축

3. 현재 단어 목록에서 두 개의 단위를 결합하여 단어 단위 목록을 1씩 증가시켜 새로운 단어 단위를 생성. 모델에 추가될 때 훈련 데이터에 대한 우도를 가장 많이 증가시키는 모든 가능한 단어 중에서 새 단어 단위를 선택

4. 사전 정의된 단어 단위 제한에 도달하거나 가능성 증가가 특정 임계값 아래로 떨어질 때까지 2로 이동

<img src="https://Ki-Wing.github.io/assets/img/ai_paper/token/4.png" alt="2" width="100%"/>

<br>

## 동작 코드

- huggingface 사용

```python
from transformers import AutoTokenizer
from collections import defaultdict

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

print(tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token) # [CLS] [SEP] [PAD]
print(tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id) # 101 102 0

text = 'hello~ my name is yena'

res = tokenizer(text)
print(res)  # {'input_ids': [101, 19082, 199, 1139, 1271, 1110, 6798, 1605, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
res = tokenizer.tokenize(text)
print(res)  # ['hello', '~', 'my', 'name', 'is', 'ye', '##na']
res = tokenizer.encode(text)
print(res)  # [101, 19082, 199, 1139, 1271, 1110, 6798, 1605, 102]
res=tokenizer.decode(res)
print(res)  # [CLS] hello ~ my name is yena [SEP]

```

<br>

- 응용 코드

```python
word_freqs = defaultdict(int)
for text in corpus:
    words_with_offsets = tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(text)
    new_words = [word for word, offset in words_with_offsets]
    for word in new_words:
        word_freqs[word] += 1

word_freqs
```

<img src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/41c091a1-1694-403d-b70e-96754f98c026" alt="3" width="100%"/>


```python
alphabet = []
for word in word_freqs.keys():
    if word[0] not in alphabet:
        alphabet.append(word[0])
    for letter in word[1:]:
        if f"##{letter}" not in alphabet:
            alphabet.append(f"##{letter}")

vocab = ["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"] + alphabet.copy()

splits = {
    word: [c if i == 0 else f"##{c}" for i, c in enumerate(word)]
    for word in word_freqs.keys()
}

def compute_pair_scores(splits):
    letter_freqs = defaultdict(int)
    pair_freqs = defaultdict(int)
    for word, freq in word_freqs.items():
        split = splits[word]
        if len(split) == 1:
            letter_freqs[split[0]] += freq
            continue
        for i in range(len(split) - 1):
            pair = (split[i], split[i + 1])
            letter_freqs[split[i]] += freq
            pair_freqs[pair] += freq
        letter_freqs[split[-1]] += freq

    scores = {
        pair: freq / (letter_freqs[pair[0]] * letter_freqs[pair[1]])
        for pair, freq in pair_freqs.items()
    }
    return scores

pair_scores = compute_pair_scores(splits)
for i, key in enumerate(pair_scores.keys()):
    print(f"{key}: {pair_scores[key]}")
    if i >= 5:
        break

best_pair = ""
max_score = None
for pair, score in pair_scores.items():
    if max_score is None or max_score < score:
        best_pair = pair
        max_score = score

def merge_pair(a, b, splits):
    for word in word_freqs:
        split = splits[word]
        if len(split) == 1:
            continue
        i = 0
        while i < len(split) - 1:
            if split[i] == a and split[i + 1] == b:
                merge = a + b[2:] if b.startswith("##") else a + b
                split = split[:i] + [merge] + split[i + 2 :]
            else:
                i += 1
        splits[word] = split
    return splits

vocab_size = 70
while len(vocab) < vocab_size:
    scores = compute_pair_scores(splits)
    best_pair, max_score = "", None
    for pair, score in scores.items():
        if max_score is None or max_score < score:
            best_pair = pair
            max_score = score
    splits = merge_pair(*best_pair, splits)
    new_token = (
        best_pair[0] + best_pair[1][2:]
        if best_pair[1].startswith("##")
        else best_pair[0] + best_pair[1]
    )
    vocab.append(new_token)

def encode_word(word):
    tokens = []
    while len(word) > 0:
        i = len(word)
        while i > 0 and word[:i] not in vocab:
            i -= 1
        if i == 0:
            return ["[UNK]"]
        tokens.append(word[:i])
        word = word[i:]
        if len(word) > 0:
            word = f"##{word}"
    return tokens

def tokenize(text):
    pre_tokenize_result = tokenizer._tokenizer.pre_tokenizer.pre_tokenize_str(text)
    pre_tokenized_text = [word for word, offset in pre_tokenize_result]
    encoded_words = [encode_word(word) for word in pre_tokenized_text]
    return sum(encoded_words, [])
```

- 실행

```python
text = "Hello, how are you?"
tokens = tokenizer.tokenize(text) 
indexed_tokens = tokenizer.convert_tokens_to_ids(tokens)
print(tokens)
print(indexed_tokens)

```

[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
