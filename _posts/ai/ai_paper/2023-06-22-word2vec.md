---
title: Efficient Estimation of Word Representations in Vector Space
author: kiwing
date: 2023-06-22 15:49:30 +0800
categories: [AI, Paper]
tags: [Paper]
pin: false
---
# [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)

# introduction

1. One-Hot Encoding

- A method of setting the size of a set of words as the dimension of a vector, assigning a value of 1 to the index of the word to be expressed, and assigning 0 to other indices

- The number of words is proportional to the dimension size

- Hard to see similarities between words

2. Distributed Representation

- Expressing the meaning of a word by distributing it across multiple dimensions

- Words that occur in similar contexts tend to have similar meaning

<br>

**goals of paper**

- Proposal of technology to learn high-level word vectors from large amounts of data

  (Rather than grouping similar words together in a vector space, they represent similar things from multiple perspectives.)

- vector(”King”) - vector(”Man”) + vec- tor(”Woman”) results in a vector that is closest to the vector representation of the word Queen

      O = E X T X Q

E : number of the training epochs <br>
T : number of the words in the training set <br>
Q : defined by each model structure <br>

<br>

# Previous work

1. Feedforward Neural Net Language Model (NNLM)

<img width="100%" alt="1" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a1.png">

      Q = N X D + N X D X H + H X V

- Fixed the number of words to be used as history // consider future words ❌

2. Recurrent Neural Net Language Model (RNNLM)

<img width="100%" alt="2" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a2.png">

      Q = H X H + H X V

<br>

# Word2Vec

New Log-linear Models : propose two model structures for learning distributed representations while minimizing computational complexity

**"Cute short legged puppy is dancing"**

1. Continuous Bag-of-Words Model (CBOW)

<img width="80%" alt="3" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a3.png">

- Past word order has no effect on projection // Use future words

      Q = N × D + D × log2(V)

2. Continuous Skip-gram Model

<img width="80%" alt="4" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a4.png">

- Since the similarity decreases as the distance increases, the words that are far away are given less sampling during training, so the weight is small

      Q = C × (D + D × log2(V))

<br>

# Basic

**Hierarchical Softmax**

- A methodology for approximating softmax, which significantly reduces the amount of computation of the existing softmax

<img width="100%" alt="4" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a5.png">

- σ(x) + σ(-x) = 1

- CBOW : v^ = (Vx^(c−m) + ⋯ + Vx^(c+m)) / 2m

- Skip-gram :  vc = Vx

<img width="100%" alt="6" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a6.png">

<br>

<img width="100%" alt="6" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a7.png">

<br>

# Results

<img width="60%" alt="6" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a8.png">

- Syntatic and semantic similarity of two words can be measured through a very simple algebraic operation

  -> vector("biggest") - vector("big") + vector("small") = vector("smallest")

- Test

  - Test set of 5 types of semantic questions, 9 types of syntactic questions // 8,869 semantic and 10,675 syntactic questions
 
  - Manually select similar words
 
  - Pairing two words together creates a large list of questions

- Evaluation

  - If the word closest to the predicted result vector matches the question perfectly, it is treated as the correct answer

  - 100% Accuracy ❌
 
      **" It is better to set more data and more Vector Dimension at the same time large. "**

<img width="100%" alt="6" src="https://Ki-Wing.github.io/assets/img/ai_paper/word2vec/a9.png">

- Compare different model structures using the same training data using 640-dimensional word vectors

[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
