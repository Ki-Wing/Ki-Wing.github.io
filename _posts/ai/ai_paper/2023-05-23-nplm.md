---
title: A Neural Probabilistic Language Model
author: kiwing
date: 2023-05-23 13:15:00 +0800
categories: [AI, Paper]
tags: [Paper]
pin: false
---
# [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)

Keywords: Statistical language modeling, artificial neural networks, distributed representation, curse of dimensionality

**1. Language model**

- A model that assigns probabilities to word sequences (sentences) to model a phenomenon called language

- Vocab(vocabulary) : Lists the tokenized words in the text file

<br>

**Statistical Language Model, SLM**

<img width="100%" alt="2" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_01.png">


- Represented by the conditional probability of the next word given all the previous ones

- N-gram models construct ta- bles of conditional probabilities for the next word, for each one of a large number of contexts, i.e. combinations of the last n − 1 words

<img width="100%" alt="3" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_02.png">

- count(A) / count(B)  =>    Multiply each probability value to obtain the word appearance probability

<img width="100%" alt="4" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_03.png">

-> Sparsity Problem

   (Problem of zero probability for a new word combination)

<br>

**N-gram**

- A method of representing a series of consecutive elements in natural language processing. Used primarily in text data, it represents n consecutive tokens (characters, words, etc.)

   " The cat is walking in the bedroom "

1. unigrams : the, cat, is, walking, in, the, bedroom

2. bigrams : the cat, cat is, is walking, walking in, in the, the bedroom

3. trigrams : the cat is, cat is walking, is walking in, walking in the, in the bedroom

4. 4-grams : the cat is walking, cat is walking in, is walking in the, walking in the bedroom

<br><br>

1. Additive(Laplace) Smoothing

- To solve the zero probability problem by removing zero count by adding a constant

<img width="80%" alt="6" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_05.png">

2. Back-off Smoothing

- Used to handle cases where the probability of a higher order n-gram is zero or very low

- ex. Tri-gram zero probability value  --change-->  Bi-gram

3. Smoothed(Interpolated) Model

- Combine the probabilities of several n-gram models (1-gram, 2-gram, 3-gram, etc.) to calculate the final probability

<img width="100%" alt="7" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_04.png">

<br><br>

**2. Word representation**

- One-Hot Encoding

   -> Represent categorical data as numerical data // simplest way to represent a word as a vector
   
   -> [바나나, 사과, 복숭아, 노트북]   =>   [1,0,0,0], [0,1,0,0] ...
   
<br>
   
- Distributed Representation

<img width="100%" alt="8" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_06.png">

   -> Expressing words with vectors consisting of successive real numbers

   -> relationship between words // less sensitive to changes in value

<br><br>

**3. Neural Probabilistic LM**

- Indicates the joint probability function for the sequence (for language model)

- Learn the embedded feature of each word in the vocabulary (for word representation)

<img width="100%" alt="9" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_07.png">

- Generalizable for sentences with similar structures or roles  =  Semantically & syntactically similar 

<br>

**Input**

<img width="100%" alt="input" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_08.png">

[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]

- Vocab size = 10, Embed size = 3, Hidden size = 6

**Hidden**

<img width="100%" alt="hidden" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_09.png">

**Output**

<img width="100%" alt="output" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_10.png">

**Optional**

<img width="100%" alt="output" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_11.png">

- Params

   -> Lookup table : Consists of the number of vocabularies and each word representation dimension
   
   -> Network param : y = b + Wx + U * tanh(d + Hx)
   
   (The goal of the model is to find parameters that maximize the log-like hood for the learning data)

<br>

- Perplexity

<img width="80%" alt="output" src="https://Ki-Wing.github.io/assets/img/ai_paper/nlpm/nplm_12.png">

   -> A measure of the level at which the language model is struggling to find the next token
   
   -> Perplexity = exp(cross-entropy) 

   -> Cross-entropy : measure of the difference between the distribution of actual data (p(x)) and the predicted probability distribution of the model (q(x))   -> -Σ p(x) * log(q(x))

[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
