---
title: Chap01
author: kiwing
date: 2023-03-17 20:55:00 +0800
categories: [AI, NNDL]
tags: [NNDL]
pin: false
img_path: '/posts/20180809'
---
# CHAPTER1. Using neural nets to recognize handwritten digits

<p align="center"><img width="80%" alt="1" src="https://user-images.githubusercontent.com/127064655/225781408-3c875a59-d2b4-42b9-a589-aa18b938f713.png"></p>

- Neural network is a system that can 'learn' large numbers of handwriting called training examples.

- Neural network leverage data to infer handwriting recognition rules on their own, and the larger the number of learning datasets, the more handwriting can be learned, thereby increasing the accuracy.


## Perceptrons

- Perceptrons were developed in the 1950s and 1960s by the scientist Frank Rosenblatt, inspired by earlier work by Warren McCulloch and Walter Pitts.

<p align="center"><img width="80%" alt="2" src="https://user-images.githubusercontent.com/127064655/225781476-07d2707d-f16e-4bcd-94f7-d8a5248730bd.png"></p>

- A perceptron takes several binary inputs, x1,x2,…, and produces a single binary output

- The neuron's output, 0 or 1, is determined by whether the weighted sum ∑jwjxj is less than or greater than some threshold value.

- Effective in binary classification problems.

<p align="center"><img width="80%" alt="3" src="https://user-images.githubusercontent.com/127064655/225783176-52e3102a-21c4-43ba-bef6-cdae5787de3f.png"></p>

<p align="center"><img width="80%" alt="4" src="https://user-images.githubusercontent.com/127064655/225783567-aaeb10dc-e043-4257-aa7c-fdf627c40d73.png"></p>

- Simplify the way we describe perceptrons 

  1) To write ∑jwjxj as a dot product, w⋅x≡∑j wjxj, 
     
     (where w and x are vectors whose components are the weights and inputs, respectively)

  2) To move the threshold to the other side of the inequality, and to replace it by what's known as the perceptron's bias, b ≡−threshold.

- Failed to learn complex functions such as XOR, but advanced models such as multilayer perceptrons have emerged to address this problem.


## Sigmoid neurons

<img width="100%" alt="2" src="https://user-images.githubusercontent.com/127064655/225781476-07d2707d-f16e-4bcd-94f7-d8a5248730bd.png">

- Just like a perceptron, the sigmoid neuron has inputs, x1,x2,…. But instead of being just 0 or 1, these inputs can also take on any values between 0
 and 1.
 
- Sigmoid neurons have wieght and bias, but the output is "σ(w·x+b)" instead of 0 or 1.

- 'σ' called the sigmoid function

<img width="80%" alt="5" src="https://user-images.githubusercontent.com/127064655/225785899-e0742743-e89d-4ce6-9655-7f63e55285b0.png">

<img width="80%" alt="6" src="https://user-images.githubusercontent.com/127064655/225785965-be02a40f-abe7-4a48-88cd-1ac4dcf7fdc3.png">

- Let's assume that z≡w⋅x+b is a very large positive number. Then, e^−z≈0 and σ(z)≈1. This means that when z=w⋅x+b is a very large positive number, the output of a sigmoid neuron is approximately 1. 

- Let's assume that z=w⋅x+b is a very small negative number. Then, e^−z→∞ and σ(z)≈0. This means that when z=w⋅x+b is a very small negative number, the output of a sigmoid neuron is approximately 0. Therefore, when z is a very small negative number, a sigmoid neuron is roughly similar to a perceptron.

<img width="100%" alt="7" src="https://user-images.githubusercontent.com/127064655/225786644-95523ce1-f344-4697-8e99-a49311083fa2.png">

- The amount of change in the output value for the input variables w and b.

- The change in median value is Δwj. The amount of change in deflection is Δb. j is an index.

- Sigmoid neurons differ from perceptrons in that their output is not just 0 or 1.


## The architecture of neural networks

<p align="center"><img width="100%" alt="8" src="https://user-images.githubusercontent.com/127064655/225787370-6dabc643-aa33-48be-b2d6-235d8da12825.png"></p>

- The leftmost layer in this network is called the input layer, and the neurons within the layer are called input neurons.

- The rightmost or output layer contains the output neurons, or, as in this case, a single output neuron. 

- The middle layer is called a hidden layer, since the neurons in this layer are neither inputs nor outputs.

<img width="80%" alt="9" src="https://user-images.githubusercontent.com/127064655/225787633-eb18864a-a892-478d-84b0-2b67fb08e5bd.png">

- Multiple layer networks are sometimes called multilayer perceptrons or MLPs, despite being made up of sigmoid neurons, not perceptrons.

- Feedforward neural networks : This means there are no loops in the network - information is always fed forward, never fed back.

(Information delivery is one-way)

<br>     
<br>  

## A simple network to classify handwritten digits

<p align="center"><img width="100%" alt="10" src="https://user-images.githubusercontent.com/127064655/225788660-15f208a7-ae78-4017-ac48-1ec17a708e3d.png"></p>

- First, we'd like a way of breaking an image containing many digits into a sequence of separate images, each containing a single digit.

- Second, classifying individual digits

<p align="center"><img width="100%" alt="11" src="https://user-images.githubusercontent.com/127064655/225789510-5351d0ae-34db-4359-bb97-17ba5ea1d49b.png"></p>

- The input layer of a neural network consists of neurons encoding input pixel values. (28 * 28)

- The input pixel is gray scale greyscale, a value of 0.0 represents white, and 1.0 represents black

- The second layer of the neural network is the hidden layer. The number of neurons in the hidden layer is called 'n'.

- The output layer of the neural network includes 10 neurons. 

  (Number the output neurons from 0 to 9, and find out which neuron has the highest activation value.)
  
- **Why is the deeper the neural network better?**

  1) More complex troubleshooting: The more or deeper the neural nw, the more complex the problem is solved. This is because each layer can learn       different features of d and detect more complex patterns.
  
  2) Better performance: The more or deeper the neural nw, the better performance. It learns more data and makes more accurate predictions.
 
 -> <span style='background-color: #fff5b1'>It is important to select the appropriate depth and width!!!</span>


## Learning with gradient descent

- Use the MNIST data set, which contains tens of thousands of scanned images of handwritten digits, together with their correct classifications.

<p align="center"><img width="100%" alt="12" src="https://user-images.githubusercontent.com/127064655/225792333-e13a3238-b224-4cc5-ae75-90c62044670d.png"></p>

- x : training input // 28 * 28 = 784 -dimensional V

- y=y(x) : desired output // 10 -dimensional V

- For every learning data input x, an algorithm is needed to find weights and biases that make the output of the neural network approximate y(x). 
  (Define "cost function" to quantify how well this goal has been reached.)
  
<img width="100%" alt="13" src="https://user-images.githubusercontent.com/127064655/225793467-a4b7d4e9-8252-4b77-ada3-d9d54bf14aa5.png">

- Σ : Sum for all learning data inputs x.

- a : Output of neural network when input is x (actual correct value)

- Obtain the mean square error (MSE) for all data samples.

- The goal is to minimize cost C(w,b), which is a function of weight and bias.    ->  USE "Gradient Descent"

- **Why use gradient descent for a cost functions?**

  -> Gradient Descent : Use the slope of the function to minimize the value of the function. The slope of the function represents the direction in which      it decreases most rapidly from its current position.
  
  -> The slope descent method updates the parameter values in the direction of minimizing the cost function and is used to improve the performance of the model. Finding parameter values with minimal cost functions is directly related to increasing the accuracy of the model.


-**How can apply gradient descent to learn neural networks?**

1) Define the cost function

- The cost function measures how well the neural network performs on the training data. It is defined as the average of the errors between the predicted outputs and the true outputs of the training examples. The cost function is typically denoted by J.

2) Compute the gradients

- Compute the gradients of the cost function with respect to the weights and biases of the network. This can be done using backpropagation, which is a recursive algorithm that computes the gradients from the output layer to the input layer.

3) Update the w and b

<p align="center"><img width="100%" alt="13" src="https://user-images.githubusercontent.com/127064655/225811345-eedbde8d-2611-4056-81a9-856b750ac1c6.png"></p>

4) Repeat for multiple iterations

- Repeat steps 2 and 3 for multiple iterations until convergence.

- Gradient descent is used to learn neural networks by computing the gradients of the cost function with respect to the weights and biases of the network, and then updating the weights and biases in the opposite direction of the gradient. This process is repeated for multiple iterations until convergence.


-**Stochastic Gradient Descent(SGD)**

-> Calculates gradients and updates parameters only for randomly selected mini-batch from a dataset. 

-> Learn faster than using any data set. 

-> Take more time to reach the optimum point, and learning may become unstable if a random sample is some extreme value.
  

## Implementing our network to classify digits
```
 git clone https://github.com/mnielsen/neural-networks-and-deep-learning.git
```

[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
