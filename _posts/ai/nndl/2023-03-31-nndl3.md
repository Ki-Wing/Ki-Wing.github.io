---
title: Chap03
author: kiwing
date: 2023-03-31 20:55:00 +0800
categories: [AI, NNDL]
tags: [NNDL]
pin: false
img_path: '/posts/20180809'
---

# CHAPTER3. Improving the way neural networks learn

- A better choice of cost function, known as the cross-entropy cost function; four so-called "regularization" methods (L1 and L2 regularization, dropout, and artificial expansion of the training data), which make our networks better at generalizing beyond the training data; a better method for initializing the weights in the network; and a set of heuristics to help choose good hyper-parameters for the network. 

<br>

## The cross-entropy cost function

<img width="100%" alt="1" src="https://user-images.githubusercontent.com/127064655/230244009-6e5541c2-d154-4802-a1c1-f95e55bdecab.png">

<br> <br>

<img width="100%" alt="2" src="https://user-images.githubusercontent.com/127064655/230245011-6f0036ee-325e-4311-882c-2c1d59861813.mp4">

- (w = 0.6, b = 0.9, n = 0.15)

- output = sigmoid(input(x) * weight + bias)

- sigmoid(x) = 1 / (1 + exp(-x))

<img width="100%" alt="3" src="https://user-images.githubusercontent.com/127064655/230246121-bc18b5a1-3c31-4a12-9adb-f3f99508a449.mp4">
- (w = 2.0, b = 2.0, n = 0.15)

<br> <br>

- "Learning is slow" == Partial derivatives are small

<img width="100%" alt="4" src="https://user-images.githubusercontent.com/127064655/230249518-0368a7f2-c600-42a8-a8dd-ba8b812a72e1.png">

- a : neuron's output(training input x=1)    //    y = desired output

- a = σ(z)

- z = wx + b

<p align="center"><img width="100%" alt="5" src="https://user-images.githubusercontent.com/127064655/230250518-20ef8e66-97a3-4daa-ab85-681e7bd6c094.png"></p>

- σ(z) = 1 / (1 + e^(-z))

- σ'(z) = σ(z) * (1 - σ(z))

<p align="center"><img width="70%" alt="6" src="https://user-images.githubusercontent.com/127064655/230256124-296db8d5-4ba0-4543-a21b-7f9b2672e5c3.png"></p>

-> When the neuron's output is close to 1, the curve gets flat, and so σ′(z) gets small

<br>

## Introducing the cross-entropy cost function

<p align="center"><img width="100%" alt="7" src="https://user-images.githubusercontent.com/127064655/230267237-843a3764-c472-4cc5-a209-0f60e5ff259f.png"></p>

- Output: a = σ(z)

- Weighted sum of the inputs :  z=∑ⱼwⱼxⱼ+b

<p align="center"><img width="100%" alt="8" src="https://user-images.githubusercontent.com/127064655/230269434-44b7277e-8eda-4dff-85b9-96531581b440.png"></p>

<br>

**+) Two properties in particular make it reasonable to interpret the cross-entropy as a cost function.**

1) Non-negative

  - All the individual terms in the sum are negative, since both logarithms are of numbers in the range 0 to 1

  - Minus sign out the front of the sum
  
2) Close to zero if the actual output for x is approximate to the desired output

  - ex. y=0 and a≈0 -> −ln(1−a) ≈ 0

-> Summing up : Cross-entropy is positive, and tends toward zero as the neuron gets better at computing the desired output, y, for all training inputs, x

<br>

<p align="center"><img width="100%" alt="9" src="https://user-images.githubusercontent.com/127064655/230273339-2f5a4dd2-1dba-48ca-93dd-86aee83e2f9b.png"></p>

- ∂C/∂wj = ∂C/∂a * ∂a/∂z * ∂z/∂wj

<p align="center"><img width="100%" alt="10" src="https://user-images.githubusercontent.com/127064655/230279126-76f26130-9e9f-4db4-a832-fa21edfe5d45.png"></p>

<p align="center"><img width="10050%" alt="11" src="https://user-images.githubusercontent.com/127064655/230279916-f5ad99de-107d-4a17-9762-48ed99512f23.png"></p>

- σ′(z) = σ(z)(1−σ(z)) 

- The larger the error, the faster the neuron will learn

<br>

<p align="center"><img width="100%" alt="mp4" src="https://user-images.githubusercontent.com/127064655/230281499-91577902-ecc8-464b-a026-286d531c3b23.mp4"></p>
- (w = 0.6, b = 0.9, n = 0.15)

<p align="center"><img width="100%" alt="mp4" src="https://user-images.githubusercontent.com/127064655/230281553-778768e5-5583-4d1c-9c7e-743d4cbe691f.mp4"></p>
- (w = 2.0, b = 2.0, n = 0.15)

<br>

**When should we use the cross-entropy instead of the quadratic cost?**

- Cross entropy should be used instead of square cost for classification problems. This is because cross-entropy gives the model a larger penalty when the model makes false predictions with confidence, leading to a higher probability assignment to the correct class and a lower probability assignment to the wrong class. 

- The square cost is better suited for regression problems. This is because the square cost treats all errors equally and does not impose a large penalty for false predictions than cross-entropy.
<br>

## Using the cross-entropy to classify MNIST digits

=code=

<br>

## Overfitting and regularization

<p align="center"><img width="100%" alt="g1" src="https://user-images.githubusercontent.com/127064655/230300646-0ff53d18-59de-4f70-8d74-a863bc24e6b1.png"></p>

- [784, 30, 10] // training_data[:1000], 400, 10, 0.5

<p align="center"><img width="100%" alt="g2" src="https://user-images.githubusercontent.com/127064655/230301167-04f436de-268a-4c37-8c53-05e384351762.png"></p>

<br>

<p align="center"><img width="100%" alt="g3" src="https://user-images.githubusercontent.com/127064655/230304210-828c229c-62d5-4ec0-bb13-181f34f97dad.png"></p>

<br>

## Regularization

- weight decay / L2 regularization : Add an extra term to the cost function, a term called the regularization term

<p align="center"><img width="100%" alt="12" src="https://user-images.githubusercontent.com/127064655/230308183-be5d4344-b787-4abf-b896-bcd2cda0a4c9.png"></p>

- First term : usual expression for the cross-entropy

- Second term : sum of the squares of all the weights in the network

- λ : regularization parameter

<p align="center"><img width="100%" alt="13" src="https://user-images.githubusercontent.com/127064655/230310055-9fcbbc6a-d1b7-476d-992d-17d75a71b955.png"></p>

- C0 : unregularized cost function

- regularization can be viewed as a way of compromising between finding small weights and minimizing the original cost function

- when λ is small we prefer to minimize the original cost function, when λ is large we prefer small weights

<br>

<p align="center"><img width="100%" alt="14" src="https://user-images.githubusercontent.com/127064655/230315992-cba6b798-f663-40db-848c-65058b052a27.png"></p>

<p align="center"><img width="100%" alt="15" src="https://user-images.githubusercontent.com/127064655/230316387-f6318226-5a9b-4ead-a19f-c1cb2a06f481.png"></p>

- This rescaling is sometimes referred to as weight decay, since it makes the weights smaller.

<p align="center"><img width="100%" alt="16" src="https://user-images.githubusercontent.com/127064655/230317817-c50f4784-d977-4871-af1d-394cf1e90d18.png"></p>

- Sum : over training examples x in the mini-batch

- Cx : (unregularized) cost for each training example.

<br>

<p align="center"><img width="100%" alt="g1" src="https://user-images.githubusercontent.com/127064655/230320641-26ef8720-bf89-426d-af73-7519424b97a7.png"></p>

- [784, 30, 10] // training_data[:1000], 400, 10, 0.5, lmbda = 0.1

<p align="center"><img width="100%" alt="g2" src="https://user-images.githubusercontent.com/127064655/230320906-46cbca2d-4e92-45f9-bbd3-5f8982bace8f.png"></p>

<br> <br>

<p align="center"><img width="100%" alt="g3" src="https://user-images.githubusercontent.com/127064655/230323555-25cc24d1-b42f-4196-a1e9-86e9f3b14efa.png"></p>

- [784, 30, 10] // training_data, 30, 10, 0.5, lmbda = 5.0

<br>

**Why does regularization help reduce overfitting?**

-> To help the model learn more general patterns by limiting the complexity of the model and making the weight smaller.

-> It is important to select the appropriate hyperparameter value.

<br><br>

## Other techniques for regularization

**1) L1 regularization**

- Modify the unregularized cost function by adding the sum of the absolute values of the weights

<p align="center"><img width="100%" alt="17" src="https://user-images.githubusercontent.com/127064655/230332292-e470eb96-7318-4c09-ba37-be23fb67f5a0.png"></p>

- penalizing large weights, and tending to make the network prefer small weights

<p align="center"><img width="100%" alt="18" src="https://user-images.githubusercontent.com/127064655/230373144-c5c388dd-1861-4b34-85c6-999aaa83f994.png"></p>

* w > 0, sgn(w) = 1
* w < 0, sgn(w) = -1
* w = 0, sgn(w) = 0

<br>

**L1 vs L2**

<p align="center"><img width="100%" alt="ex1" src="https://user-images.githubusercontent.com/127064655/230810850-b6089934-c723-4c6a-b328-f2e4ee0760a1.png"></p>

- L1 regularization(Lasso), adds a penalty term to the cost function of a model, which is proportional to the absolute value of the coefficients of the model. It encourages sparsity in the model by pushing the coefficients of less important features towards zero. As a result, L1 regularization can be used for feature selection and results in a sparse model where only the most important features are retained.

- L2 regularization(Ridge), adds a penalty term to the cost function of a model, which is proportional to the square of the coefficients of the model. It discourages large weights in the model, resulting in a model with smaller, more evenly distributed weights. L2 regularization is particularly effective when all the features are potentially relevant to the target variable.

**Summing up** : L1 regularization leads to sparse models with a small number of non-zero coefficients, whereas L2 regularization leads to models with smaller weights that are more evenly distributed across all features.

<br>

**2) Dropout**

<p align="center"><img width="100%" alt="19" src="https://user-images.githubusercontent.com/127064655/230379859-05c89a43-bd26-4ce6-8346-fdbc53d77738.png"></p>

- Modify the network itself

- Randomly select and exclude some neurons from the learning process

<br> 

**3) Artificially expanding the training data**

<p align="center"><img width="100%" alt="20" src="https://user-images.githubusercontent.com/127064655/230385840-949b51f8-224e-4097-9a7d-6ed0814392d7.png"></p>

- Using more learning data significantly improves classification accuracy

<p align="center"><img width="100%" alt="21" src="https://user-images.githubusercontent.com/127064655/230387346-d9bd2a4a-f504-4e5f-9eac-b600bbec499a.png"></p>

<br>

**Summing up**: We've now completed our dive into overfitting and regularization. Of course, we'll return again to the issue. As I've mentioned several times, overfitting is a major problem in neural networks, especially as computers get more powerful, and we have the ability to train larger networks. As a result there's a pressing need to develop powerful regularization techniques to reduce overfitting, and this is an extremely active area of current work.

<br> <br>

## Weight initialization

- Use initialization as a Gaussian random variable with a mean of 0 and a standard deviation of 1 divided by the square root of the number connected to the neuron

<p align="center"><img width="100%" alt="22" src="https://user-images.githubusercontent.com/127064655/230401597-4212f639-98de-4af8-b81a-8396d38058f8.png"></p>

- Adjusting the median to be appropriately small

-> Less chance of losing gradients early in learning

- Learn faster

-> The square root value is reduced, resulting in a decrease in the size of the gradient.

## Handwriting recognition revisited

-code-

<br>

## How to choose a neural network's hyper-parameters?

**Learning rate**

<p align="center"><img width="100%" alt="23" src="https://user-images.githubusercontent.com/127064655/230521610-52f0044d-a3fc-4688-9f79-2b548bff9dd1.png"></p>

- [784, 30, 10] // training_data, 30, 10, n, lmbda = 5.0

- Too large a learning rate does not diverge or converge

- Too small a learning rate, the learning will be too slow (take a long time to converge)

**Use early stopping to determine the number of training epochs**

- Stop learning when the model is overfitted

- Track the model's generalization error using validation data

**Learning rate schedule**

- Adjust learning rates between epochs or etherations as learning progresses.

- Tech

-> Constant learning rate: initialize learning rates and do not change during training

-> Less Learning Rate: Select Inital Learning Rate and then gradually decrease with Scheduler

**Mini-batch size**

- Too small Mini-batch size, to take full advantage of a good matrix library optimized for fast hardware

- Too large does not update the weights often enough

**Automated techniques**

- Bayesian Optimization 

  - Assuming an unknown objective function (f(x)) that receives an input value (x), we find the optimal solution that maximizes the function value (f(x)) 
   
  -> black-box function

  - Surrogate Mode, Acquisition Function

<br>

**Softmax?**

- If the total number of choices is k, the probability for each class is estimated by receiving a vector of k dimensions

- Calculates the exponential of each input value and then normalizes the resulting values by dividing them by the sum of all exponential values. This ensures that the output values are between 0 and 1, and that their sum is equal to 1, making them interpretable as probabilities.

**Summing up** : Mathematical function that transforms a vector of numbers into a probability distribution that can be used for multiclass classification tasks.

<p align="center"><img width="100%" alt="24" src="https://user-images.githubusercontent.com/127064655/230827425-c6fdc42d-a5ae-44d9-b7a8-ccac584accd9.png></p>
