---
title: Chap04
author: kiwing
date: 2023-04-07 20:55:00 +0800
categories: [AI, NNDL]
tags: [NNDL]
pin: false
img_path: '/posts/20180809'
---
# CHAPTER4. A visual proof that neural nets can compute any function

http://neuralnetworksanddeeplearning.com/chap4.html

- Neural networks have a kind of universality. No matter what function we want to compute, we know that there is a neural network which can do the job

<br>

## Two caveats

1) First, this doesn't mean that a network can be used to exactly compute any function

- Can have as many good approximations as you want

- Increasing the number of hidden neurons 

2) The second caveat is that the class of functions which can be approximated in the way described are the continuous functions

- If a function is discontinuous, it cannot usually be approximated using a neural network

- Available if the function is discontinuous but has a good continuous approximation

**Summing up** : More precise statement of the universality theorem is that neural networks with a single hidden layer can be used to approximate any continuous function to any desired precision.

<br>

## Universality with one input and one output

 <p>&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;<img width="60%" alt="1" src="https://user-images.githubusercontent.com/127064655/230849518-3fb766b9-96c2-4658-93a0-889a3c467d26.png">&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;&ensp;<img width="60%" alt="2" src="https://user-images.githubusercontent.com/127064655/230849618-30eba5e3-37e2-4e43-a04e-362694a9d7a5.png"></p>

- σ(wx + b)

- σ(z) ≡ 1 / (1 + e^-z)

<p align="center"><img width="90%" alt="3" src="https://user-images.githubusercontent.com/127064655/231702952-b79dde7b-2cd7-4655-9fa7-64012f4a6813.png"></p>

-  s = −b/w    // step position

- proportional to b, and inversely proportional to w

<p align="center"><img width="90%" alt="4" src="https://user-images.githubusercontent.com/127064655/231702429-a2fdc815-e8d9-403b-a420-a717b70d22d6.png"></p>

- w1a1 + w2a2    // a : neurons' activations.

- "BUMP" function

<p align="center"><img width="90%" alt="5" src="https://user-images.githubusercontent.com/127064655/231708721-22b3057f-6739-4bf6-8d7e-32809b7df810.png"></p>

<p align="center"><img width="90%" alt="6" src="https://user-images.githubusercontent.com/127064655/231709452-9a19139a-142c-4e60-828e-c947e9ff8964.png"></p>

- f(x) = 0.2 + 0.4x^2 + 0.3xsin(15x) + 0.05cos(50x)

<p align="center"><img width="90%" alt="7" src="https://user-images.githubusercontent.com/127064655/231915791-544d18c6-768e-4b87-952d-00dd3c4394d0.png"></p>

<br>

<p align="center"><img width="90%" alt="8" src="https://user-images.githubusercontent.com/127064655/231916540-d127a7ce-04e0-4136-8807-546118acb150.png"></p>

**Is there some way we can achieve control over the actual output from the network?**

- σ(∑jwjaj+b) 

- Design a neural network whose hidden layer has a weighted output given by σ^−1∘f(x)    // σ^−1 is just the inverse of the σ function

- Average deviation : The average difference between the target function and the function calculated by the network

<br>

**Increasing the number of bumps and hidden neuron pairs?**

- Increasing the number of neurons in the hidden layer increases the diversity of functions that neural networks can express. Therefore, when the number of neurons is small, the complexity of the approximate function is reduced.

- Increasing the number of bumps allows you to divide the intervals of the functions you want to approximate in more detail, resulting in more accurate approximation results.

<br>

**Standard parameterization used for neural networks**

- The first layer of weights all have some large, constant value

- The biases on the hidden neurons are just b = −ws

- The final layer of weights are determined by the h values

- Finally, the bias on the output neuron is 0

**Summing up**

- Prove that an artificial neural network with one input and one output can approximate any continuous function

- Universality Theorem : A neural network with one hidden layer can approximate any continuous function if it has a sufficient number of neurons

- Set the number of neurons in the hidden layer large enough, and use nonlinear activation functions such as sigmoid functions to give nonlinearity to neural networks

<br>

## Many input variables

<p align="center"><img width="90%" alt="9" src="https://user-images.githubusercontent.com/127064655/231940927-3b72e909-6a7f-40c4-b739-5f85b81af96b.mp4"></p>

- sx ≡ −b/w1

<p align="center"><img width="90%" alt="10" src="https://user-images.githubusercontent.com/127064655/231946906-41fa704c-cb6a-4e57-b4b0-cb70ba8b1809.png"></p>

<p align="center"><img width="90%" alt="11" src="https://user-images.githubusercontent.com/127064655/231958528-4e889ce3-88d8-44ab-a5ec-78d221c4ee20.png"></p>

- Sums two bump functions, one in the x-axis direction, the other in the y-axis direction, and both in height h

- When two bump functions are combined, the values of the two functions are added in the area where each function affects, resulting in a combined function

<p align="center"><img width="90%" alt="12" src="https://user-images.githubusercontent.com/127064655/231961576-5db33f51-2b30-4f44-b305-a5d954682213.png"></p>

<br>

## Extension beyond sigmoid neurons

<p align="center"><img width="90%" alt="13" src="https://user-images.githubusercontent.com/127064655/231983252-2c23403d-602b-4b93-a4b6-767cfa2f8e8b.png"></p>

- σ(∑jwjxj + b)   // wj are the weights, b is the bias, and σ is the sigmoid function

<br>

<p><img width="70%" alt="11" src="https://user-images.githubusercontent.com/127064655/231985730-b05f6538-ae37-4424-865a-05c201170734.png">   <img width="80%" alt="11" src="https://user-images.githubusercontent.com/127064655/231985766-2e5f3a1d-d284-4fc2-a3ab-88d73e1007d2.png"></p>

- s(∑jwjxj + b)    // z→−∞ and z→∞, s(z)

<br>

## Fixing up the step functions

<p align="center"><img width="80%" alt="13" src="https://user-images.githubusercontent.com/127064655/232675059-f71ffa7b-166b-4d3c-8dc3-db3f1c9a9538.png"></p>

- Failure window : An interval in which the form of an activation function produced by a neuron or set differs from the correct step function

<p><img width="80%" alt="13" src="https://user-images.githubusercontent.com/127064655/232722153-2adcdbd7-10ff-4c40-b418-42b888c6dc95.png"><img width="80%" alt="13" src="https://user-images.githubusercontent.com/127064655/232722085-3bc8d691-95e6-4a01-9119-9890e331190c.png"></p>

<br> <br>

## Conclusion

**universality theorem**

- The theory that any function can be approximated in a neural network large enough

- Approximate any function by adjusting the weight and activation function of neurons

    1. the universality theorem does not mean that a neural network can exactly calculate any function. While the network can approximate any function to any desired degree of accuracy, there may be some functions that are impossible to represent exactly. This is because the neural network uses a finite number of neurons, and thus can only approximate a function with a finite number of parameters

    2. the types of functions that can be approximated are limited to continuous functions. Discontinuous functions, such as step functions, are not necessarily approximable by neural networks. However, it is often possible to approximate discontinuous functions with a series of continuous functions that approach the desired function as the number of terms in the series approaches infinity

[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
