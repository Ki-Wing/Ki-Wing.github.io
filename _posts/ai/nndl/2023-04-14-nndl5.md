---
title: Chap05
author: kiwing
date: 2023-04-14 16:55:00 +0800
categories: [AI, NNDL]
tags: [NNDL]
pin: false
img_path: '/posts/20180809'
---
# CHAPTER5. Why are deep neural networks hard to train?

<p align="center"><img width="90%" alt="1" src="https://user-images.githubusercontent.com/127064655/234142869-947b94fb-8156-4611-b71a-03e4c0913d30.png"></p>

- Deep Neural Network has a very deep structure that can handle complex problems. As the input data passes through multiple hidden layers, it extracts various features, and combines them to learn complex patterns. 

- Deep neural networks often have higher accuracy and better predictive power than shallow networks.

<br>

## The vanishing gradient problem

<p align="center"><img width="80%" alt="2" src="https://user-images.githubusercontent.com/127064655/234153767-c2c80b65-8acd-48ba-ac4d-c149c8b3ccc8.png"></p>

- [784,30,30,10]   //  bar = gradients for each neuron(∂C/∂b)

- Second hidden layer bars are mostly much larger than first hidden layer bars.

- δlj = ∂C/∂blj    //  ‖δ1‖=0.07... and ‖δ2‖=0.31...


<p align="center"><img width="100%" alt="3" src="https://user-images.githubusercontent.com/127064655/234159543-3ba6d434-410b-4bd9-ad0d-e588bbf67281.png"></p>

-[784,30,30,10]

<p align="center"><img width="100%" alt="4" src="https://user-images.githubusercontent.com/127064655/234159581-53ecbd9a-6f39-46ca-b4d8-18f48694b82b.png"></p>

-[784,30,30,30,10]

<p align="center"><img width="100%" alt="5" src="https://user-images.githubusercontent.com/127064655/234159631-34680146-0505-47c7-88e1-943574b06915.png"></p>

-[784,30,30,30,30,10]

<br>

## What's causing the vanishing gradient problem? Unstable gradients in deep neural nets

<p align="center"><img width="100%" alt="6" src="https://user-images.githubusercontent.com/127064655/234180194-507150d5-d400-413a-86f2-99ac59e3fe42.png"></p>

- Output aj from the jth neuron is σ(zj), where σ is the usual sigmoid activation function, and zj=wjaj−1+bj is the weighted input to the neuron.

- Cost is low if the actual output of the network is close to the desired output, and cost is high if it is far away.

<br>

<p align="center"><img width="100%" alt="7" src="https://user-images.githubusercontent.com/127064655/234181234-117b9b43-4645-4a47-b55f-d88b280ad297.png"></p>

- ∂C/∂b1 ≈ ΔC/Δb1

- a1 = σ(z1) = σ(w1a0 + b1) 

<p align="center"><img width="100%" alt="8" src="https://user-images.githubusercontent.com/127064655/234187111-5cbd1b65-6d74-401b-a4ca-c7cb0f8e42c8.png"></p>

- Converts a change Δb1 in the bias into a change Δa1 in the output activation

- Δa1 in turn causes a change in the weighted input z2 = w2a1 + b2 to the second hidden neuron 

- Δz2≈(∂z2/∂a1)Δa1 = w2Δa1

<p align="center"><img width="100%" alt="9" src="https://user-images.githubusercontent.com/127064655/234189375-e19e4b4e-5961-4f1a-a3ee-ee5b5e10b788.png"></p>

<p align="center"><img width="100%" alt="10" src="https://user-images.githubusercontent.com/127064655/234195412-5ab50332-55da-4801-a8a0-19bfeaae9214.png"></p>

<br>

**Why the vanishing gradient problem occurs**

<p align="center"><img width="100%" alt="11" src="https://user-images.githubusercontent.com/127064655/234195993-bfa9c36b-6362-4f43-b2d8-65329de07606.png"></p>

- ∂C/∂b1 = σ'(z1) * w2 * σ'(z2) * w3 * σ'(z3) * w4 * σ'(z4) * ∂C/∂a4    // wjσ'(zj) 

- Max : σ′(0) = 1/4

<p align="center"><img width="100%" alt="12" src="https://user-images.githubusercontent.com/127064655/234205496-e0838a12-9ef7-468b-b358-bbdec629af3f.png"></p>

- Gradient ∂C/∂b1 will usually be a factor of 16 (or more) smaller than ∂C/∂b3

**The exploding gradient problem**

1. Choose all the weights in the network to be large (w1=w2=w3=w4=100)

2. Choose the biases so that the σ′(zj) terms are not too small. 

-> The real problem here is that neural networks suffer from an unstable gradient problem
 
**The prevalence of the vanishing gradient problem**

|wσ′(z)|≥1 // w : σ′(z)=σ′(wa+b)

[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
