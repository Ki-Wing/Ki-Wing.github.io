---
title: Chap02
author: kiwing
date: 2023-03-24 20:55:00 +0800
categories: [AI, NNDL]
tags: [NNDL]
pin: false
img_path: '/posts/20180809'
---
# CHAPTER2. How the backpropagation algorithm works

weight =  w      // bias =  b     // network =  nw    // cost function =  C

<br>

## (Warm up) a fast matrix-based approach to computing the output from a neural network

- fast matrix-based algorithm 

  1. Calculate the product of the input data and w matrix     // matrix operations
  
  2. Adds a b matrix to the multiplication matrix of input data and w
  
  3. Apply the activation function to the result of b added to the product of the input data and w

<p align="center"><img width="100%" alt="1" src="https://user-images.githubusercontent.com/127064655/226768251-3f61128e-a07c-4af0-a186-6ac8f115c96d.png"></p>

- wᴸⱼₖ  //      j : jth neuron in the lth layer     k : kth neuron in the (l−1)th layer

<img width="100%" alt="2" src="https://user-images.githubusercontent.com/127064655/226809993-8ba620b1-820f-4fae-a575-464a6eb9ad7d.png">

- The process by which neurons in each layer are activated by receiving input values.

- Calculate the value output by the jth neuron in the lth layer.

<img width="100%" alt="3" src="https://user-images.githubusercontent.com/127064655/226816821-9476e321-9be6-4a22-8b72-6e2b61627a66.png">

- zᴸ ≡ wᴸ a(ᴸ⁻¹) + bᴸ 

  - zᴸ : weighted input to the neurons in layer l

  - aᴸ = σ(zᴸ) 
  
<br>

## The two assumptions we need about the cost function

- The goal of backpropagation is to compute the partial derivatives ∂C/∂w and ∂C/∂b of C with respect to any w or b in the nw

<img width="100%" alt="4" src="https://user-images.githubusercontent.com/127064655/226825724-cfb54771-6d5a-4303-b83c-ab756e631f9b.png">

- n : Total number of learning data //  Σ : Sum of x for individual learning data

- y=y(x) :  desired output  //  aL=aL(x) : Vector of activations output from the nw when x is input

<br>

**1. The cost function can be written as an average C=(1/n)∑x Cx over cost functions Cx for individual training examples, x**
  
  - Assume for a quadratic C. 

  - single training example : Cx = (1/2)‖y−aL‖^2

  -> Calculating and averaging C of all learning data obtain the overall C.
  
<br>

**2. The cost is that it can be written as a function of the outputs from the neural network.**

<img width="100%" alt="5" src="https://user-images.githubusercontent.com/127064655/226841156-e2206b95-8f73-4b07-b53e-f133e6bbe500.png">

  - The optimization of the model parameters can be facilitated because the change in the output value directly affects C.
  
  -> C used to learn the predictive model can be represented as a function of calculating the cost by receiving the output values of the neural nw as input.
  
<br>

**_These assumptions train the model well by properly defining the cost function, and make the test data show high performance._**

<br>

## The four fundamental equations behind backpropagation

- Backpropagation is about understanding how changing w and b in a network changes C. // ∂C/∂wᴸⱼₖ and ∂C/∂bᴸⱼ

<img width="100%" alt="6" src="https://user-images.githubusercontent.com/127064655/227070978-7982ebb4-9230-45d6-89d5-aafdee83fbaf.png">

- C differentiated by the input value zᴸⱼ of the jth neuron in the lth layer

- zᴸⱼ : Current layer input value

- δᴸⱼ : Measuring the difference between the output value of a neuron and the actual value (measuring the magnitude of the error)

<br>

**1. An equation for the error in the output layer**

<img width="100%" alt="7" src="https://user-images.githubusercontent.com/127064655/227076709-c84c7a67-377f-4fa2-94cc-7c236751d5b2.png">

- ∂C/∂aᴸⱼ : Measures how fast the cost is changing as a function of the jth output activation.

  -> C = 1/2∑j(yⱼ − aᴸⱼ)^2
  
  -> ∂C/∂aᴸⱼ = (aᴸⱼ − yⱼ)

- σ′(zᴸⱼ) : Measures how fast the activation function σ is changing at zᴸⱼ.

<img width="100%" alt="8" src="https://user-images.githubusercontent.com/127064655/227080158-972e79da-6d12-4877-9ebd-ad13902eca7f.png">

- δᴸ : Error in output layer

- ∇aC : Gradient vector of output layer activation function 'a' for C

**2. An equation for the error δⱼ in terms of the error in the next layer**

<img width="100%" alt="9" src="https://user-images.githubusercontent.com/127064655/227099750-8066ba62-bd67-49e5-9c4e-64923d1108e4.png">

- Error δᴸ of the current layer l is calculated by multiplying the error δ(l+1) of the next layer by the differential value σ'(zl) of the output value of the current layer

**3. An equation for the rate of change of the cost with respect to any bias in the network**

<img width="100%" alt="10" src="https://user-images.githubusercontent.com/127064655/227111450-4b94bfa8-79e9-4d8b-bd69-05ea0e3f8b29.png">

- The error δ is exactly equal to the rate of change ∂C/∂b

- Formula used in all layers of the neural network. // Effect of the deflection blj of the jth neuron in the lth layer on C.

**4. An equation for the rate of change of the cost with respect to any weight in the network**

<img width="100%" alt="11" src="https://user-images.githubusercontent.com/127064655/227145617-19afa735-1ae9-449b-a1ed-56beef566945.png">

- Rate of change in the C for w of the neural network

- ain : Activation of the neuron input to // δout : Error of the neuron output from w

-  W(B) will learn slowly when the input neuron is inactive or the output neuron has saturated.

<br>

<p align="center"><img width="100%" alt="12" src="https://user-images.githubusercontent.com/127064655/227153452-7393d9a0-04f4-46d0-847e-8666b94be01d.png"></p>

<br>

## The backpropagation algorithm

<p align="center"><img width="100%" alt="13" src="https://user-images.githubusercontent.com/127064655/227391532-c0147c70-d481-48de-8537-20ec734ef88b.png"></p>

- Calculate the differential value by moving backward in the output layer, uses it to adjust the w and b of each layer. 

<p align="center"><img width="100%" alt="14" src="https://user-images.githubusercontent.com/127064655/227392322-769e55c1-4066-4891-8a39-1174f814c22e.png"></p>

- Learning neural networks by using mini-batch to calculate errors for multiple training examples at a time and by using gradient descent to update w and b.

## In what sense is backpropagation a fast algorithm?

- Enables us to simultaneously compute all the partial derivatives '∂C/∂wj' using just one forward pass through the network, followed by one backward pass through the network. 

- The cost of calculating backwards is about the same as the cost of forwarding.

## Backpropagation: the big picture

[what's the algorithm really doing?]

<p align="center"><img width="100%" alt="15" src="https://user-images.githubusercontent.com/127064655/227428509-a8ad2c74-8c14-4913-8a7e-2dccb1705937.png"></p>

1. Small change Δwᴸⱼₖ to some w in the network, wᴸⱼₖ

2. That change in weight will cause a change in the output activation from the corresponding neuron.

3. That, in turn, will cause a change in all the activations in the next layer

4. Those changes will in turn cause changes in the next layer, and then the next, and so on all the way through to causing a change in the final layer, and then in the cost function

<img width="100%" alt="16" src="https://user-images.githubusercontent.com/127064655/227434953-ac84655a-f546-4d38-896d-64007b8c85a0.png">

- Approximate the change in C with a small change in the w parameter wᴸⱼₖ

<img width="100%" alt="17" src="https://user-images.githubusercontent.com/127064655/227436997-873224e0-a14f-4a5c-99ec-ee247d30a82b.png">


<img width="100%" alt="18" src="https://user-images.githubusercontent.com/127064655/227437509-915f24b4-e27f-4a1b-9e33-043cba6e3b8f.png">

[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
