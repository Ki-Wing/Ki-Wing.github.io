---
title: Transformer
author: kiwing
date: 2023-10-13 19:36:39 +0800
categories: [AI, Model]
tags: [Model]
pin: false
---

# [Attention is all you need](https://arxiv.org/pdf/1706.03762)

[TDB]

# Abstract
- 주요 시퀀스 변환 모델은 인코더와 디코더를 포함하는 복잡한 순환 또는 컨벌루션 신경망을 기반으로 한다
- 최고 성능의 모델은 attention 메커니즘을 통해 인코더와 디코더를 연결한다.
- 우리는 반복과 컨볼루션을 완전히 없애고 attention 메커니즘 에만 기반한 새로운 간단한 네트워크 아키텍쳐인 transformer를 제안했다
  -> "Transformer" : 온전히 attention mechanism에만 기반한 구조. (recurrence 나 convolution은 사용하지 않음) // 더 parallelizable하고, 훨씬 적은 학습 시간이 걸림
- 두가지 기계 번역 작업에 대한 실헙에서는 이러한 모델이 품질면에서 우수하면서도 병렬화가 더 용이하고 학습에 소요되는 시간이 훨씬 적은 것으로 나타났다
- transformer가 크고 제한된 훈련 데이터를 사용하여 영어 선거구 구문 분석에 성공적으로 적용함으로써 다른 작업에 잘 일반화된다는 것을 보여준다.

# Introduction
- rnn, lstm은 언어 모델링 및 기계 번역과 같은 시퀀스 모델링 및 변환 문제에 대한 최첨단 접근 방식으로 확고히 자리 잡았다. 이후 순환 언어 모델과 인/디코더 아키텍처의 경계를 넓히기 위한 수많은 노력이 계속되었다
- 순환 모델은 일반적으로 입/출력 시퀀스의 기호 위치에 따른 계산을 고려한다. 계산 time step에 따라 위치를 정렬하면 이전 hidden state의 ht-1및 위치 t에 대한 입력의 함수로 일련의 ht가 생성된다.
- 이러한 본직적인 시퀀스 특징으로 인해 훈련 예제 내에서 병렬화가 불가능하며, 이는 메모리 제약으로 인해 예제 간 일괄 처리가 제한되므로 시퀀스 길이가 길어질수록 중요해진다.
- 최근 연구에서는 인수분해 트릭과 조건부 계산을 통해 계산 효율성이 크게 향상되었으며 후자의 경우 모델 성능도 향상되었찌만 순차 계산의 근본적인 제약은 여전히 남아있다
- Attention mechanisms은 다양한 작업에서 강력한 시퀀스 모델링 및 변환 모델의 필수적인 부분이 되어 입/출력 시퀀스의 거리에 관계없이 종속성을 모델링할 수 있다.
- 그러나 몇몇 경우를 제외한 모든 경우에, Attention mechanisms은 순환 nw와 함께 사용된다. 이 연구에서 우리는 반복을 피하고 대신 Attention mechanisms에 전적으로 의존하여 입/출력 사이 전역 종속성을 그리는 모델 아키텍처인 transformer를 제안한다
- transformer는 뤌씬 많은 병렬화를 허용하며 8개의 P100 GPU에서 단 12시간 동안 교육을 받은 후 번역 품질에서 새로운 최첨단 기술에 도달했다.

"""
Sequence Modeling과 transduction 문제에서 RNN, long-term memory, gated RNN이 SOTA를 달성해 옴
 
Recurrent model은 parallelization이 불가능해 longer sequence length에서 치명적임
최근 연구에서 factorization trick과 conditional computation을 통해 계산 효율성을 많이 개선
특히 conditional computation은 모델 성능도 동시에 개선
-> 그러나 여전히 근본적인 sequential computation의 문제는 남아있음
 
Attention mechanism은 다양한 분야의 sequence modeling과 transduction model에서 주요하게 다뤄짐
Attention mechanism은 input과 output sequence간 길이를 신경쓰지 않아도 됨
-> 하지만 여전히 recurrent network와 함께 사용되었음
 
Transformer
input과 output간 global dependency를 뽑아내기 위해 recurrence를 사용하지 않고, attention mechanism만을 사용함
-> parallelization이 가능해 적은 시간으로 translation quality에서 SOTA를 달성할 수 있었음

"""

# Background
- 순환 계산을 줄이는 목표는 또한 확장 신경망 GPU, ByteNet 및 ConvS2S의 기초를 형성한다. 이 모두는 컨볼루션 신경망을 기본 빌딩 블록으로 사용하고 모든 입력 및 데이터에 대해 hidden representation을 병렬로 계산한다. 
  - byteNet

      1.수행시간이 시퀀스 길이에 대해 linear 타임으로 증가하고, 시퀀스의 시간 해상도를 유지한다
    
      2.디코더는 캐릭터 레벨의 language modeling에 있어서 state-of-the-art 성능을 낸다 (RNN으로 만든 기존 버전들보다 낫다)
    
      3.인코더가 만든 representation의 latent alignment structure가 실제 시퀀스들간의 예상되는 순서를 반영한다.
    
      -> 걍 두개의 CNN을 쌓은 것이다. (인코더/디코더) 하나는 source sequence를 인코딩하고, 하나는 디코딩하여 target sequence를 만들어낸다
    
      -> ByteNet이라는 neural translation model과 ByteNet Decoder라는 neural language model을 제시해서 이 두가지 단점을 극복 (밑에는 2가지 단점)
    
        = RNN은 본질적으로 연속적인 구조를 가지고 있기 때문에 시퀀스를 따라 패럴럴하게 작동할 수가 없다. RNN의 포워드와 백워드 시그널들은 전체 시퀀스 길이를 다 횡단해야만 한 지점에서 다른 지점으로 갈 수 있다. 길이가 길어지면 지점들 사이 상관관계를 학습하기 힏들다.
    
        = 여러 nearal architecture가 제시되었지만 이들은 소스와 타켓 시퀀스의 길이에 대해 'super linear'한 수행식낭르 갖거나, 소스 시퀀스를 contant한 사이즈의 representation으로 프로세싱하기 때문에 메모리화 과정이라는 짐을 지우게 된다. 이 두가지 단점은 학습시킬 시퀀스의 길이가 길어지면 더 심해진다.

      -> ByteNet은 encoder-decoder 모델로서, 확장된 CNN을 사용해서 소스 네트워크와 타겟네트워크를 구성한다. 두가지 중요한 특징을 가짐
      
      -> Encoder로부터 나온 representation에 대해 decodering 스택해서 쌓는 구조. 이 때, 시간resolution을 유지하도록 하는 구조. 다양한 길이의 source, target sequence를 다루기 위한 dynamic unfolding이라는 방법

- Intra-attention이라고도 불리는 Self-attention은 시퀀스의 표현을 계산하기 위해 단일 시퀀스의 다양한 위치와 관련된 주의 메커니즘
- Self-attention은 독해, 추상적 요약, 텍스트 수반 및 작업 독립적인 문장 표현 학습을 포함한 다양한 작업에서 성공적으로 사용
- End-to-end memory networks는 시퀀스 정렬 재발 대신 recurrent attention mechanism을 기반으로 하며 간단한 언어 질문 응답 및 언어 모델링 작업에서 잘 수행
-  txfmer은 시퀀스 정렬 rnn 또는 컨볼루션을 사용하지 않고 입력 및 출력 표현을 계싼하기 위해 전적으로 self-attention에 의존하는 최초의 변환 모델

"""

- sequential computation을 줄이는 것은 Extended Neural GPU, ByteNet, ConvS2S에서도 다뤄짐
- 이 연구들은 모두 CNN을 basic building block으로 사용함
- input output 거리에서 dependency를 학습하기 어려움
-> Transformer에서는 Multi-Head Attention 으로 상수 시간으로 줄어듦 
 
- self-attention(=intra-attention)
- reading comprehension, abstractive summarization, testual entailment, learning task, independent sentence representations를 포함한 다양한 task에서 성공적으로 사용됨

- End to End memory network
- sequence-aligned recurrence 보다 recurrent attention mechanism에 기반한다
- simple-language question answering과 lauage modeling task에서 좋은 성능을 보인다
- transformer는 온전히 self-attention에만 의존한 최초의 transduction model이다 (sequnce-aligned RNN이나 Convolution을 사용하지 않음)

"""


# Model Architecture
- 가장 경쟁력 있는 neural sequnce transduction model은 인코더-디코더 구조를 가지로 있다. 여기서 인코더는 입력 기호 표현 시퀀스(x1...xn)를 연속표현 시퀀스 z = (z1,...,zn)에 매핑한다
- z가 주어지면 디코더는 한 번에 한 요소씩 심볼의 출력 시퀀스(y1,...ym)을 생성한다. 각 단계에서 모델은 자동 회귀적이며, 이전에 생성된 기호를 다음 기호를 생성할 때 추가 입력으로 사용한다.
- txfer는 논문 figure1의 왼쪽과 오른쪽에 각각 표시된 것처럼 인코더/디코더 모두에 대해 누적된 self-attention 및 포인트별 fc를 사용하는 전체 아키텍처를 따른다. 

1) Encoder and Decoder Stacks

![image](https://github.com/ynlee-hs/test/assets/127064655/396da0f3-55d4-4f60-8072-d3cc08396681)

**Encoder**

- Encoder는 6개의 identical layer로 이루어짐
- 각 layer는 두 개의 sub-layer 가 있음
- 첫번째 sub-layer는 multi-head self-attention mechanism
- 두번째 sub-layer는 간단한 Position-wise fully connected fedd-forward network
- 각 two sub-layer 마다 layer normalization 후에 residual connection을 사용
- 즉 각 sub-layer의 결과는 Layer Norm(x + sublayer(X))
- residual connection을 구현하기 위해, embedding layer를 포함한 모든 sub-layer들의 output은 512 차원임

**Decoder**
- 6개의 identical layerfh dlfndjwla
- 각 Encoder layer의 두 sub-layer에, decoder는 세번재 sub-layer를 추가함
  -> encoder stack의 결과에 해당 layer가 multi-head attention을 수행함
- 마찬가지로 residual connection 적용

+) residual connection
-> 컴퓨터 비전 분야에서의 ResNet 모델과 자연어 처리 분야에서의 transformer 모델에서 더 좋은 성능을 내기 위해 사용되었다. residual connection은 아주 deep한 신경망에서 하위 층에서 학습된 정보가 데이터 처리 과정에서 손실되는 것을 방지하기 위한 방법

"""

[Exploding gradient & Vanishing gradient problems]

= 층이 많은 신경망에서 gradient problem이 일어나는 이유를 한 줄로 요약해보자면, 레이어가 많아질수록 weight value가 아주 작아지거나 아주 커지기 때문이다. 그리고 이러한 현상은 모델 파라미터가 최적해로 안정적으로 수렴하지 못하게 하는 원인이 된다.

= Residual connection은 일부 레이어를 건너뛰어 데이터가 신경망 구조의 후반부에 도달하는 또 다른 경로를 제공함으로써 gradient가 계속 커지거나 작아지는 문제를 해결

"""

# Attention

- Attention function은 query와 key-value쌍을 output에 매핑함(query, key, value, output모두 vector임)
- output은 value들의 weighted sum으로 계산됨

1. Scaled Dot-Product Attention

![image](https://github.com/ynlee-hs/test/assets/127064655/d7e6f646-2300-49b4-b612-e16b3ab8b1c4)

- Input :  query,  key의 dimension dk, value의 dimension dv
- 모든 쿼리와 key에 대해 dot product을 계산하고, √dk로 나눠, weight을 적용하기 위해 value에 softmax함수를 적용
- Attentnion(Q,K,V) = softmax(QK^T / √dk) V
<br> <br>
- 두가지 Attention function이 있음
  
1) Additive attention : single hidden layer로 feed-forward layer network를 사용해 compatibility function을 계산
2) Dot-product attention : scaling factor (1 / √dk) 를 제외하면 이 연구에서의 attention 방식과 동일

- dk가 작으면 두 방식의 성능은 비슷하지만, dk가 큰 경우 additive 가 더 성능이 좋음
- dk가 크면 dot product의 경우 gradient가 너무 작아지는 문제를 해결하기 위해 dot product를 1/√dk로 스케일링함


2. Multi-Head Attention

![image](https://github.com/ynlee-hs/test/assets/127064655/c72907b7-0dab-49c8-885e-b96f1de71a40)

- single attention을 dmodel-dimensional keys, values, queries에 적용하는 것보다, queries, keys, values를 h번 서로 다른, 학습된 linear projection으로 dk, dk와 dv 차원에 Linear하게 project하는게 더 효과적이라는 사실을 알아냄

-> project된 각 값들은 병렬적으로 attention function을 거쳐, dv-dimensional output vlaue를 만들어 냄

-> 이 결과들은 다시 합쳐진 다음, 다시 한번 project되어 최종 결과값을 만듦

![스크린샷 2023-10-10 오전 9 26 24](https://github.com/ynlee-hs/test/assets/127064655/8423115f-2f5d-4306-a45d-6d5fca703a4b)

이 연구에선 h = 8, dk = dv = dmodel/h = 64
-> 각 head마다 차원을 줄이기 때문에, 전체 계산 비용은 전체 차원에 대한 single-head attention과 비슷함

3. Applications of Attention in our Model
- Transformer은 3가지 방법으로 multi-head attentiondmf tkdydgka

1) 인코더/디코더 attention layers에서
   
-> query는 이전 디코더 layer에서 나옴

-> memory key와 value는 인코더의 output에서 나옴 // 따라서 디코더의 모든 position이 input sequence의 모든 position을 다룸

-> 전형적인 sequence to sequence model에서의 인코더-디코더 attention 방식임

2) 인코더는 self-attention layer를 포함하고 있음

-> self-attention layer에서 key, value, query는 모두 같은 곳(인코더 이전 layer의 output)에서 나옴

-> 인코더의 각 position은 인코더의 이전 layer의 모든 position을 다룰 수 있음

3) decoder 또한 self-attetnion layer를 가짐

-> 마찬가지로, 디코더의 각 position은 해당 position까지 모든 position을 다룰 수 있음

-> 디코더의 leftforward information flow는 auto-regressive property 때문에 막아줭 할 필요성이 있음 // 이 연구에서 scaled-dot product attention에서 모든 softmax의 input value 중 illegal connection에 해당하는 값을 -∞ 로 masking out해서 구현함

# Position-wise Feed Forward Networks
- 인코더 디코더의 각 layer는 fully connected feed-forward network를 가짐
- 이는 각 position에 따로따로 동일하게 적용됨
- relu 활성화 함수를 포함한 두 개의 선형 변환이 포함됨

![스크린샷 2023-10-10 오전 9 44 03](https://github.com/ynlee-hs/test/assets/127064655/0cf9a47c-1d85-4a2e-bee6-6d81778535a5)

- linear transformation은 다른 position에 대해 동일하지만 layer간 parameter는 다르다.

# Embedding and softmax
- 다른 seq transduction model처럼, 학습된 임베딩을 사용한다
- input 토큰과 output 토큰을 dmodel의 벡터로 변환하기 위함

- decoder output을 예측된 다음 토큰의 확률로 변환하기 위해 선형 변환과 softmax를 사용함
- transformer에서는, 두개의 임베딩 layer와 pre-softmax 선형 변환 간, 같은 weight의 matrix를 공유한다

- 임베딩 layer에서는 weight들에 √dmodel을 곱해줌

# Positional Encoding
- Transformer는 어떤 recurrent, convolution도 사용하지 않기 때문에, sequence의 순서를 사용하기 위해 seq의 상대적, 절대적 position에 대한 정보를 주입해줘야 함


[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
