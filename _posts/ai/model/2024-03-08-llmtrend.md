---
title: LLM trend 조사
author: kiwing
date: 2024-03-08 22:59:35 +0800
categories: [AI, Model]
tags: [Model]
pin: false
---

# LLM Trend Research

- LLM (Large Language Model)

## 1.LLaMa (Meta)
https://ai.meta.com/blog/large-language-model-llama-meta-ai/
- 2023.02.24
- Model Parallelism: 6.7B(1-way), 13B(2-way), 32.5B(4-way), 65.2B(8-way)
- Access: Open source
- 1.4 T tokens로 학습

<img width="100%" alt="1" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/32d05917-7d0a-495f-ac36-9d43f48534cc">

<br>

## 1-1.Llama 2 (Meta)
https://ai.meta.com/blog/llama-2/
- 2023.07.18
- Parameters: 7 billion, 13 billion, 70 billion
- Access: Open source

<img width="100%" alt="2" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/5c3f5895-0303-4984-889a-7686f1836283">

- Llama 2는 기존 버전보다 40% 더 많은 2조 개의 토큰으로 학습(1.4T→2T 토큰).
- 컨텍스트 길이도 2배 증가한 4096으로 설정(2K→4K).
- 무료로 상업적 이용 가능. 단, 월간 활성 사용자(MAU)가 7억 명의 회사가 활용할 경우 meta와 별도 라이센스 계약 필요

<img width="100%" alt="3" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/07b30733-320b-483d-9f05-ff1d1efe1d5e">

- SFT(Supervised Fine-Tuning)과 RLHF이 모두 사용

<br>

## 1-2.Vicuna (LMSYS Org)
https://lmsys.org/blog/2023-03-30-vicuna/
- 2023.03.30
- Parameters: 7 billion, 13 billion, and 33 billion
- Access: Open source
- Vicuna-13B는 Meta의 LLaMA와 Stanford의 Alpaca에 영감을 받아 UC Berkeley, UCSD, CMU, MBZUAI(Mohamed Bin Zayed Univ. of AI)가 공동으로 개발한 오픈소스 챗봇으로, ShareGPT로 부터 수집된 사용자들의 대화로 LLaMA를 fine-tuning한 모델
- 공개 API를 사용하여 ShareGPT에서 수집한 약 70K의 사용자 대화로 LLaMA을 fine-tuning

<img width="100%" alt="4" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/0ceb6ad2-fbce-44e4-b24b-1d3853dc75fe">

[Relative Response Quality Assessed by GPT-4]

- it has not been sufficiently optimized to guarantee safety or mitigate potential toxicity or bias. To address the safety concerns, we use the OpenAI moderation API to filter out inappropriate user inputs in our online demo. (최적화 x)

<br>

- 질문: 왕실 연회에서 중세 기사로서 자신을 어떻게 소개하시겠습니까?

LLaMA-13b의 응답:

> "나는 왕에게 충성하는 용감한 기사입니다. 왕의 명예를 지키고 그의 나라를 지키기 위해 최선을 다하겠습니다. 

비쿠나-13b의 응답:

> 중세 기사로서 나는 왕실 연회에서 나의 미천한 출신과 왕관에 대한 충성심을 인정하면서 깊은 인사와 정중한 연설로 내 자신을 소개하곤 했습니다. 나는 다음과 같이 말할 것입니다.
> "영주 여러분, 저는 겸손한 기사로서 왕국을 섬기고 수호할 것을 맹세했습니다. 저는 여러 세대에 걸쳐 이 왕국을 지키기 위해 싸워온 수많은 전사들 중 한 명입니다. 오늘 이 자리에 여러분과 함께 하게 된 것을 영광으로 생각합니다. 나는 나의 행동과 행동으로 여러분의 신뢰와 존경을 얻기를 바랍니다. 나는 나의 왕과 나의 백성을 섬기는 데 나의 칼과 방패와 나의 생명을 바칠 것을 맹세합니다."
> 그런 다음 나는 연회에 자리를 잡고 내 상급자들의 현명한 조언을 듣고 배우며 가능한 모든 방법으로 그들을 섬길 준비가 되어 있습니다."

<br>

## 2. Claude (Anthropic)
https://www.anthropic.com/
- 2023.03.14
- Access: API

| Feature      | Claude1      | Claude2      | Claude Instant       |
|--------------|--------------|--------------|----------------------|
| Parameters   | 52 billion   | Unknown (1,300억 개 이상 추정) | - |
| Capabilities | -정교한 대화 <br> -창의적인 콘텐츠 생성 | All Claude 1 features plus academic features and more.  | -캐주얼한 대화 <br> -텍스트 분석 및 요약 <br> -문서 Q&A  |
| Cost per Token | $32.68/million tokens | $32.68/million tokens | $5.51/million tokens |
| Performance  | Good at complex tasks | Excellent at complex tasks | Good at casual tasks |
| Context Window | 75,000 tokens | 100,000 tokens | 75,000 tokens |
| Description  | 매우 긴 텍스트 범위에 걸쳐 복잡한 주제와 관계에 대한 미묘한 이해를 위해 긴 문서와 대화를 요약, 분석 및 쿼리하는 데 이상적 | 텍스트 및 코드 생성과 관련된 작업을 위한 고성능 대화형 AI. <br> 문맥 이해력, 추론 능력, 코딩 능력이 이전 버전인 Claude 1.3에 비해 크게 개선됨. | Claude 2의 저사양 버전으로 빠르고 저렴함. <br> 일상 대화, 텍스트 분석, 요약, 문서 이해 등의 작업 처리하도록 설계.  <br> Claude 2 모델의 강점을 통합하고 있으며, 수학, 코딩, 추론, 안전성 등의 분야에서 훨씬 개선 |

<img width="100%" alt="5" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/c38fdfb1-0a3a-4b1e-87de-5676e73f6c76">

https://docs.anthropic.com/claude/reference/input-and-output-sizes

<br>

## 3.PaLM (Google)
- Parameters: 540 billion
- 단일 모델을 효과적으로 학습하도록 하는 Pathways system으로 학습하고 dense decoder만 이용한 Transformer 모델
- 6144 개의 TPU(구글의 GPU 칩)를 약 1500여 개의 호스트 컴퓨터를 통해 학습

<img width="100%" alt="6" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/5fb1dca6-92a0-41cf-a350-1e329592c70a">

- pre-trained on a high-quality corpus of 780 billion tokens that comprise various natural language tasks and use cases. 
- dataset includes filtered webpages, books, Wikipedia articles, news articles, source code obtained from open source repositories on GitHub, and social media conversations

<br>

## 3-1.PaLM 2 (Google)
https://ai.google/discover/palm2/
- 2023.05.10
- Parameters: 340 billion
- Access: API
- Trained on 3.6 trillion tokens.
- 주요 기능
- 다중언어 : 100개 이상의 언어에 걸쳐 다국어 텍스트를 학습. 다양한 언어의 미묘한 뉘앙스를 이해, 생성, 번역 가능
- 추론 능력, 코딩 : 논리와 상식적 추론, 수학 등에서 더욱 향상된 역량 가짐

<img width="100%" alt="6" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/5fb1dca6-92a0-41cf-a350-1e329592c70a">

[Evaluation on reasoning tasks]

## 4. Falcon (TII)
https://falconllm.tii.ae/
- 2023.05.23
- Parameters: 1.3 billion, 7.5 billion, 40 billion, 180 billion
- Access: Open source
- RefinedWeb dataset 에서 가져온 3.5 trillion으로 학습 (180 billion)
    - https://huggingface.co/datasets/tiiuae/falcon-refinedweb

<img width="100%" alt="7" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/1c941548-dd18-41e9-a67c-6ebc0a351bd1">


- 다양한 벤치마크에서 Llama 2 70B와 GPT-3.5를 능가하며, HellaSwag, LAMBADA 등에서 Google의 PaLM 2-Large와 경쟁
    - https://huggingface.co/tiiuae/falcon-180B

<br>

## 5.Gemini (Google)
https://blog.google/technology/ai/google-gemini-ai/#performance
- 2023.12.06
- Parameters: Nano available in 1.8 billion and 3.25 billion versions; others unknown
- Access: API
- 주요 기능
- 번역, 요약, 대화 등의 작업을 위한 자연어 처리
- 수학적 사고와 문제 해결
- 코드 및 문서 생성 능력 (Gemini Ultra 74.4%, GPT-4 67%)
- 이미지, 오디오 및 비디오 이해
- 다양한 도메인에서의 멀티태스킹

<br>

- 나노: 스마트폰, 특히 Google Pixel 8에서 작동하도록 설계됨. 채팅 프로그램에서 답변을 제안하거나 텍스트를 요약하는 등 외부 서버에 연결하지 않고 인공지능에 의한 효율적인 처리가 필요한 작업을 수행하도록 설계되었음. 이 소형 장치 모델에는 약 6억 개의 매개변수가 있음.
- 프로: Pro는 최신 버전의 Bard AI 챗봇과 함께 작동하도록 설계. 빠른 응답 시간을 제공하고 복잡한 쿼리를 이해할 수 있음. 중형 모델은 약 100억 개의 매개변수를 가지고 있음.
- 울트라: LLM 연구에 사용되는 널리 사용되는 학술 테스트 32개 중 30개에 대한 현재 최첨단 성능을 능가하는 최고 성능 모델이라함.  가장 크고 가장 강력한 버전인 Ultra는 1조 개 이상의 매개변수 수신. 

<img width="100%" alt="8" src="https://github.com/Ki-Wing/Ki-Wing.github.io/assets/92567807/06ff1a62-3f78-4966-acdc-6a1828db7f9b">

<br>

>> 참고(llm list table) : https://en.wikipedia.org/wiki/Large_language_model#List

[nodejs]: https://nodejs.org/
[starter]: https://github.com/cotes2020/chirpy-starter
[pages-workflow-src]: https://docs.github.com/en/pages/getting-started-with-github-pages/configuring-a-publishing-source-for-your-github-pages-site#publishing-with-a-custom-github-actions-workflow
[latest-tag]: https://github.com/cotes2020/jekyll-theme-chirpy/tags
